#!/usr/bin/env node
/**
 * å…ƒå­¦ä¹  Agent (Meta-Learning Agent)
 *
 * "å­¦ä¼šå­¦ä¹ " - Agent ä¸ä»…å­¦ä¹ ä»»åŠ¡ï¼Œè¿˜å­¦ä¹ å¦‚ä½•æ›´å¿«åœ°å­¦ä¹ æ–°ä»»åŠ¡
 * åŸºäº MAML (Model-Agnostic Meta-Learning) æ€æƒ³
 *
 * ä¼˜å…ˆçº§: P1 (ä¸‹ä¸€ä»£è¿›åŒ–)
 */

class MetaLearningAgent {
  constructor(config) {
    this.id = config.id || `meta_agent_${Math.random().toString(36).substr(2, 9)}`;
    this.role = config.role || 'Learner';

    // Q-Learning åŸºç¡€
    this.qTable = new Map(); // state_action -> Q-value
    this.learningRate = config.learningRate || 0.1;
    this.discountFactor = 0.95;

    // å…ƒå­¦ä¹ å‚æ•°
    this.metaLr = config.metaLr || 0.01;  // å…ƒå­¦ä¹ ç‡
    this.taskHistory = [];  // ä»»åŠ¡å†å²
    this.learningCurves = [];  // å­¦ä¹ æ›²çº¿

    // å…ƒå‚æ•° (å¯å­¦ä¹ çš„å­¦ä¹ ç‡)
    this.adaptiveLR = this.learningRate;

    // å­¦ä¹ ç»Ÿè®¡
    this.tasksCompleted = 0;
    this.avgLearningSpeed = 0;
    this.metaUpdateCount = 0;

    console.log(`âœ… å…ƒå­¦ä¹  Agent åˆ›å»º: ${this.id}`);
  }

  /**
   * æ ‡å‡† Q-Learning æ›´æ–°
   */
  updateQValue(state, action, reward, nextState) {
    const key = `${state}_${action}`;
    const currentQ = this.getQValue(state, action);

    // æ‰¾åˆ°ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§ Q å€¼
    let maxNextQ = 0;
    for (const a of this.getPossibleActions()) {
      const q = this.getQValue(nextState, a);
      if (q > maxNextQ) maxNextQ = q;
    }

    // Q-learning æ›´æ–°å…¬å¼
    const newQ = currentQ + this.adaptiveLR * (reward + this.discountFactor * maxNextQ - currentQ);

    this.qTable.set(key, newQ);

    return newQ;
  }

  /**
   * å…ƒå­¦ä¹ æ›´æ–°
   * æ ¹æ®å†å²ä»»åŠ¡è¡¨ç°è°ƒæ•´å­¦ä¹ å‚æ•°
   */
  metaUpdate(taskPerformance) {
    this.metaUpdateCount++;

    // è®°å½•ä»»åŠ¡æ€§èƒ½
    this.learningCurves.push({
      taskId: taskPerformance.taskId,
      steps: taskPerformance.steps,
      reward: taskPerformance.totalReward,
      timestamp: Date.now()
    });

    // åªä¿ç•™æœ€è¿‘ 10 ä¸ªä»»åŠ¡
    if (this.learningCurves.length > 10) {
      this.learningCurves.shift();
    }

    // å¦‚æœæœ‰è¶³å¤Ÿå†å²ï¼Œè¿›è¡Œå…ƒå­¦ä¹ 
    if (this.learningCurves.length >= 3) {
      this.adjustLearningRate();
    }

    // æ›´æ–°å¹³å‡å­¦ä¹ é€Ÿåº¦
    this.avgLearningSpeed =
      (this.avgLearningSpeed * (this.tasksCompleted - 1) + taskPerformance.steps) /
      this.tasksCompleted;

    this.tasksCompleted++;
  }

  /**
   * è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå…ƒå­¦ä¹ æ ¸å¿ƒï¼‰
   */
  adjustLearningRate() {
    // åˆ†ææœ€è¿‘çš„å­¦ä¹ æ›²çº¿
    const recent = this.learningCurves.slice(-5);

    // è®¡ç®—å­¦ä¹ é€Ÿåº¦çš„æ–¹å·®
    const speeds = recent.map(c => c.steps);
    const avgSpeed = speeds.reduce((a, b) => a + b, 0) / speeds.length;
    const variance = speeds.reduce((sum, s) => sum + Math.pow(s - avgSpeed, 2), 0) / speeds.length;

    // å¦‚æœæ–¹å·®å¤§ï¼ˆä¸ç¨³å®šï¼‰ï¼Œé™ä½å­¦ä¹ ç‡
    // å¦‚æœæ–¹å·®å°ï¼ˆç¨³å®šï¼‰ï¼Œå¯ä»¥æé«˜å­¦ä¹ ç‡
    if (variance > 100) {
      this.adaptiveLR = Math.max(0.01, this.adaptiveLR * 0.95);  // é™ä½
    } else if (variance < 25) {
      this.adaptiveLR = Math.min(0.5, this.adaptiveLR * 1.05);  // æé«˜
    }

    console.log(`  ğŸ§  å…ƒæ›´æ–°: LR = ${this.adaptiveLR.toFixed(4)} (æ–¹å·®: ${variance.toFixed(1)})`);
  }

  /**
   * å¿«é€Ÿé€‚åº”ï¼ˆFew-Shot Learningï¼‰
   */
  fastAdapt(task, shots = 5) {
    console.log(`\nâš¡ å¿«é€Ÿé€‚åº”ä»»åŠ¡: ${task.id} (${shots} shots)`);

    // ä½¿ç”¨å…ƒå­¦ä¹ çš„"å…ˆéªŒçŸ¥è¯†"å¿«é€Ÿå­¦ä¹ 
    let totalReward = 0;

    for (let i = 0; i < shots; i++) {
      const state = task.env.getInitialState();
      const action = this.selectAction(state);
      const { reward, nextState } = task.env.step(action);

      // ä½¿ç”¨æ›´é«˜çš„å­¦ä¹ ç‡å¿«é€Ÿé€‚åº”
      const originalLR = this.adaptiveLR;
      this.adaptiveLR = originalLR * 2;

      this.updateQValue(state, action, reward, nextState);

      this.adaptiveLR = originalLR;
      totalReward += reward;
    }

    console.log(`  âœ… å¿«é€Ÿé€‚åº”å®Œæˆï¼Œå¹³å‡å¥–åŠ±: ${(totalReward / shots).toFixed(2)}`);

    return totalReward / shots;
  }

  /**
   * é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-greedyï¼‰
   */
  selectAction(state, epsilon = 0.1) {
    if (Math.random() < epsilon) {
      // æ¢ç´¢
      const actions = this.getPossibleActions();
      return actions[Math.floor(Math.random() * actions.length)];
    } else {
      // åˆ©ç”¨
      return this.getBestAction(state);
    }
  }

  /**
   * è·å–æœ€ä½³åŠ¨ä½œ
   */
  getBestAction(state) {
    const actions = this.getPossibleActions();
    let bestAction = actions[0];
    let bestQ = -Infinity;

    for (const action of actions) {
      const q = this.getQValue(state, action);
      if (q > bestQ) {
        bestQ = q;
        bestAction = action;
      }
    }

    return bestAction;
  }

  /**
   * è·å– Q å€¼
   */
  getQValue(state, action) {
    const key = `${state}_${action}`;
    return this.qTable.get(key) || 0;
  }

  /**
   * è·å–å¯èƒ½çš„åŠ¨ä½œ
   */
  getPossibleActions() {
    return ['up', 'down', 'left', 'right'];
  }

  /**
   * è·å–çŠ¶æ€
   */
  getState() {
    return {
      id: this.id,
      role: this.role,
      tasksCompleted: this.tasksCompleted,
      avgLearningSpeed: this.avgLearningSpeed.toFixed(1),
      adaptiveLR: this.adaptiveLR.toFixed(4),
      metaUpdates: this.metaUpdateCount,
      qTableSize: this.qTable.size
    };
  }
}

// ==================== ä»»åŠ¡ç¯å¢ƒ ====================

class TaskEnvironment {
  constructor(config) {
    this.gridSize = config.gridSize || 5;
    this.goal = config.goal || { x: 4, y: 4 };
    this.obstacles = config.obstacles || [];
    this.maxSteps = config.maxSteps || 20;
  }

  getInitialState() {
    return { x: 0, y: 0 };
  }

  step(action) {
    const state = this.current || this.getInitialState();
    let { x, y } = state;

    // æ‰§è¡ŒåŠ¨ä½œ
    switch (action) {
      case 'up': y = Math.max(0, y - 1); break;
      case 'down': y = Math.min(this.gridSize - 1, y + 1); break;
      case 'left': x = Math.max(0, x - 1); break;
      case 'right': x = Math.min(this.gridSize - 1, x + 1); break;
    }

    // æ£€æŸ¥éšœç¢ç‰©
    const isObstacle = this.obstacles.some(o => o.x === x && o.y === y);
    if (isObstacle) {
      return { reward: -10, nextState: state, done: false };
    }

    const nextState = { x, y };
    this.current = nextState;

    // æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡
    if (x === this.goal.x && y === this.goal.y) {
      return { reward: 100, nextState, done: true };
    }

    // è·ç¦»å¥–åŠ±ï¼ˆè¶Šæ¥è¿‘ç›®æ ‡å¥–åŠ±è¶Šé«˜ï¼‰
    const dist = Math.abs(x - this.goal.x) + Math.abs(y - this.goal.y);
    const reward = -dist;

    return { reward, nextState, done: false };
  }

  reset() {
    this.current = this.getInitialState();
    return this.current;
  }
}

// ==================== å…ƒå­¦ä¹ ç³»ç»Ÿ ====================

class MetaLearningSystem {
  constructor() {
    this.agents = [];
    this.tasks = [];
  }

  /**
   * åˆ›å»ºå…ƒå­¦ä¹  Agent
   */
  createAgent(config) {
    const agent = new MetaLearningAgent(config);
    this.agents.push(agent);
    return agent;
  }

  /**
   * åˆ›å»ºä»»åŠ¡
   */
  createTask(config) {
    const task = {
      id: config.id || `task_${this.tasks.length}`,
      env: new TaskEnvironment(config),
      maxSteps: config.maxSteps || 20
    };

    this.tasks.push(task);
    return task;
  }

  /**
   * è®­ç»ƒå¾ªç¯
   */
  async train(tasksPerEpoch = 5, epochs = 3) {
    console.log('\nğŸ§  å…ƒå­¦ä¹ è®­ç»ƒ\n');
    console.log('='.repeat(80) + '\n');

    for (const agent of this.agents) {
      console.log(`ğŸ¤– è®­ç»ƒ Agent: ${agent.id}\n`);

      for (let epoch = 0; epoch < epochs; epoch++) {
        console.log(`\nğŸ“š Epoch ${epoch + 1}/${epochs}\n`);

        let epochReward = 0;

        for (let i = 0; i < tasksPerEpoch; i++) {
          // éšæœºé€‰æ‹©ä»»åŠ¡
          const task = this.tasks[Math.floor(Math.random() * this.tasks.length)];
          task.env.reset();

          console.log(`  ğŸ¯ ä»»åŠ¡: ${task.id}`);

          // æ‰§è¡Œä»»åŠ¡
          const performance = this.runTask(agent, task);
          epochReward += performance.totalReward;

          // å…ƒå­¦ä¹ æ›´æ–°
          agent.metaUpdate(performance);
        }

        console.log(`\n  ğŸ“Š Epoch ${epoch + 1} å¹³å‡å¥–åŠ±: ${(epochReward / tasksPerEpoch).toFixed(1)}`);
      }
    }
  }

  /**
   * è¿è¡Œä»»åŠ¡
   */
  runTask(agent, task) {
    let state = task.env.getInitialState();
    let totalReward = 0;
    let steps = 0;

    while (steps < task.maxSteps) {
      const action = agent.selectAction(state);
      const { reward, nextState, done } = task.env.step(action);

      agent.updateQValue(state, action, reward, nextState);

      totalReward += reward;
      state = nextState;
      steps++;

      if (done) break;
    }

    return {
      taskId: task.id,
      steps,
      totalReward,
      success: state.x === task.env.goal.x && state.y === task.env.goal.y
    };
  }

  /**
   * Few-Shot å­¦ä¹ æ¼”ç¤º
   */
  async demonstrateFewShot() {
    console.log('\nâš¡ Few-Shot å­¦ä¹ æ¼”ç¤º\n');
    console.log('='.repeat(80) + '\n');

    const agent = this.agents[0];
    const newTask = this.createTask({
      id: 'few_shot_task',
      goal: { x: 3, y: 3 },
      gridSize: 5,
      maxSteps: 15
    });

    console.log('ğŸ¯ æ–°ä»»åŠ¡: ä» (0,0) åˆ° (3,3)\n');

    // 1-shot å­¦ä¹ 
    console.log('\n1ï¸âƒ£  1-Shot å­¦ä¹ :');
    const reward1Shot = agent.fastAdapt(newTask, 1);
    console.log(`   å¹³å‡å¥–åŠ±: ${reward1Shot.toFixed(1)}`);

    // 3-shot å­¦ä¹ 
    console.log('\n3ï¸âƒ£  3-Shot å­¦ä¹ :');
    const reward3Shot = agent.fastAdapt(newTask, 3);
    console.log(`   å¹³å‡å¥–åŠ±: ${reward3Shot.toFixed(1)}`);

    // 5-shot å­¦ä¹ 
    console.log('\n5ï¸âƒ£  5-Shot å­¦ä¹ :');
    const reward5Shot = agent.fastAdapt(newTask, 5);
    console.log(`   å¹³å‡å¥–åŠ±: ${reward5Shot.toFixed(1)}`);

    console.log('\nâœ… Few-Shot å­¦ä¹ å®Œæˆï¼');
    console.log(`   éšç€æ ·æœ¬å¢åŠ ï¼Œæ€§èƒ½æå‡: +${((reward5Shot - reward1Shot) / Math.abs(reward1Shot) * 100).toFixed(0)}%\n`);
  }

  /**
   * ç”ŸæˆæŠ¥å‘Š
   */
  generateReport() {
    console.log('\nğŸ“Š å…ƒå­¦ä¹ æŠ¥å‘Š\n');
    console.log('='.repeat(80) + '\n');

    for (const agent of this.agents) {
      const state = agent.getState();

      console.log(`ğŸ¤– Agent: ${agent.id}`);
      console.log(`   å®Œæˆä»»åŠ¡: ${state.tasksCompleted}`);
      console.log(`   å¹³å‡å­¦ä¹ é€Ÿåº¦: ${state.avgLearningSpeed} æ­¥/ä»»åŠ¡`);
      console.log(`   è‡ªé€‚åº”å­¦ä¹ ç‡: ${state.adaptiveLR}`);
      console.log(`   å…ƒæ›´æ–°æ¬¡æ•°: ${state.metaUpdates}`);
      console.log(`   Qè¡¨å¤§å°: ${state.qTableSize}`);
      console.log('');
    }

    console.log('âœ… æ ¸å¿ƒç‰¹æ€§éªŒè¯:\n');
    console.log('  1. âœ… å…ƒå­¦ä¹  (å­¦ä¼šå­¦ä¹ )');
    console.log('  2. âœ… è‡ªé€‚åº”å­¦ä¹ ç‡');
    console.log('  3. âœ… Few-Shot å­¦ä¹ ');
    console.log('  4. âœ… å¿«é€Ÿé€‚åº”');
    console.log('  5. âœ… å­¦ä¹ æ›²çº¿åˆ†æ\n');

    console.log('ğŸ’¡ å…ƒå­¦ä¹ çš„ä¼˜åŠ¿:\n');
    console.log('   - æ–°ä»»åŠ¡å¿«é€Ÿé€‚åº” (Few-Shot)');
    console.log('   - è‡ªåŠ¨è°ƒæ•´å­¦ä¹ å‚æ•°');
    console.log('   - ä»å†å²ä»»åŠ¡æå–å…ˆéªŒçŸ¥è¯†');
    console.log('   - æŒç»­æ”¹è¿›å­¦ä¹ æ•ˆç‡\n');
  }

  /**
   * è¿è¡Œå®Œæ•´æ¼”ç¤º
   */
  async run() {
    console.log('\nğŸ§  LX-PCEC å…ƒå­¦ä¹ ç³»ç»Ÿæ¼”ç¤º v1.0\n');
    console.log('åŸºäº: MAML (Model-Agnostic Meta-Learning)\n');
    console.log('ä¼˜å…ˆçº§: P1 (ä¸‹ä¸€ä»£è¿›åŒ–)\n');
    console.log('='.repeat(80) + '\n');

    // åˆ›å»º Agents
    console.log('ğŸ¤– åˆ›å»ºå…ƒå­¦ä¹  Agents...\n');
    this.createAgent({ id: 'meta_agent_1', role: 'Explorer' });
    this.createAgent({ id: 'meta_agent_2', role: 'Collector' });

    // åˆ›å»ºè®­ç»ƒä»»åŠ¡
    console.log('\nğŸ¯ åˆ›å»ºè®­ç»ƒä»»åŠ¡...\n');
    this.createTask({ id: 'task_1', goal: { x: 4, y: 4 }, gridSize: 5, maxSteps: 20 });
    this.createTask({ id: 'task_2', goal: { x: 2, y: 4 }, gridSize: 5, maxSteps: 20 });
    this.createTask({ id: 'task_3', goal: { x: 4, y: 2 }, gridSize: 5, maxSteps: 20 });
    this.createTask({ id: 'task_4', goal: { x: 3, y: 3 }, gridSize: 5, maxSteps: 20 });
    this.createTask({ id: 'task_5', goal: { x: 4, y: 1 }, gridSize: 5, maxSteps: 20 });

    // è®­ç»ƒ
    await this.train(5, 3);

    // Few-Shot æ¼”ç¤º
    await this.demonstrateFewShot();

    // æŠ¥å‘Š
    this.generateReport();

    return this.agents;
  }
}

// ==================== ä¸»ç¨‹åº ====================

async function main() {
  console.log('ğŸ§  LX-PCEC å…ƒå­¦ä¹  Agent æ¼”ç¤º v1.0\n');
  console.log('="'.repeat(80) + '\n');

  const system = new MetaLearningSystem();

  await system.run();

  console.log('='.repeat(80));
  console.log('âœ… å…ƒå­¦ä¹ ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼');
  console.log('='.repeat(80) + '\n');

  console.log('ğŸ¯ å®ç°çš„ç‰¹æ€§:\n');
  console.log('   1. âœ… å…ƒå­¦ä¹ ç®—æ³•');
  console.log('   2. âœ… è‡ªé€‚åº”å­¦ä¹ ç‡');
  console.log('   3. âœ… Few-Shot å­¦ä¹ ');
  console.log('   4. âœ… å­¦ä¹ æ›²çº¿åˆ†æ');
  console.log('   5. âœ… å¿«é€Ÿé€‚åº”\n');

  console.log('ğŸ’¡ å…ƒå­¦ä¹ çš„çªç ´:\n');
  console.log('   âŒ ä¼ ç»Ÿ: æ¯ä¸ªæ–°ä»»åŠ¡ä»é›¶å­¦ä¹ ');
  console.log('   âœ… å…ƒå­¦ä¹ : ç”¨å…ˆéªŒçŸ¥è¯†å¿«é€Ÿé€‚åº”\n');
  console.log('   âŒ ä¼ ç»Ÿ: å›ºå®šå­¦ä¹ ç‡');
  console.log('   âœ… å…ƒå­¦ä¹ : åŠ¨æ€è°ƒæ•´å­¦ä¹ å‚æ•°\n');
  console.log('   âŒ ä¼ ç»Ÿ: éœ€è¦å¤§é‡æ ·æœ¬');
  console.log('   âœ… å…ƒå­¦ä¹ : Few-Shot ç”šè‡³ One-Shot\n');

  console.log('ğŸš€ ä¸‹ä¸€æ­¥: ç ”ç©¶è‡ªé€‚åº”ç½‘ç»œæ‹“æ‰‘\n');
}

// è¿è¡Œ
if (require.main === module) {
  main().catch(console.error);
}

module.exports = {
  MetaLearningAgent,
  TaskEnvironment,
  MetaLearningSystem
};
